# ICCVW CVAMD 2023
This implementation is intended for ICCVW CVAMD 2023, where our method has been accepted for a poster presentation.

## The description of our approach
Should you encounter data with a long-tailed distribution, it is imperative to note that the head class tends to dominate the tail class, consequently resulting in suboptimal learning outcomes. Furthermore, in multi-label datasets, the issue is further compounded by co-occurrence patterns. To effectively harness the potential of this co-occurrence phenomenon within our model, we employed the GloVe algorithm. In our pursuit of leveraging both co-occurrence information and image data associated with these labels, we embedded each label as a node within a graph using the GloVe algorithm and employed ML-GCN as our baseline framework. To adeptly manage the dataset's long-tailed distribution, we took a meticulous approach to address each facet of ML-GCN individually. Firstly, in the quest to effectively train the tail class dataset, we adopted class-balanced sampling, eschewing the simplistic approach of random sampling. Secondly, as a countermeasure against overfitting, an issue that inevitably arises due to the scarcity of tail classes, we introduced a multi-expert structure to ensemble the model. We applied the RIDE loss function to diversify the experts within the ensemble. Lastly, we substituted the conventional max pooling operation with Log-Sum-Pooling. This modification enables us to select more discerning latent vectors from the feature map extracted from the backbone. The incorporation of a single layer within the Transformer Encoder, along with the utilization of shared weights between the features extracted through LSE pooling and GLoVe embeddings, has demonstrably enhanced the performance of our model. Therefore, our methods including class balanced sampling, RIDE loss, MHA, and LSE pooling have actually improved the vanilla ML-GCN. In addition, our methods have validated that they can be a model agnostic method which can promise improvement in model performance especially for the multi label classification task.

If you have any questions about our work, please do not hesitate to contact us by emails.
